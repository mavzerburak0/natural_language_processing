{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Regular Expressions, Text Normalization, Edit Distance\n",
    "\n",
    "**Text normalization** means converting text into more convenient, standard form.<br>\n",
    "**Lemmatization** is determining that two words have the same root e.g. sang, sung, sings all have the root *sing*.<br>\n",
    "**Stemming** is a simpler form of lemmatization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "### Basic Regular Expressions \n",
    "We use square brackets ( [] ) to specify a disjunction e.g. [wW] matches both lower- and uppercase *w*.<br>\n",
    "Dash ( - ) can be used to specify any character in a range.<br>\n",
    "**ˆ character** is used to specify what the character *cannot* be.<br>\n",
    "**? character** is used to specify \"the preceding character or nothing\" e.g. *colou?r* would match color or colour.<br>\n",
    "**Kleene star** ( * ) is used to specify “zero or more occurrences of the immediately previous character or regular expression” e.g. [ab]* would match *aaaa* *ababab*.<br>\n",
    "**Kleene plus** ( + ) is useful to match “one or more occurrences of the immediately preceding character or regular expression” e.g. [0-9]+ would match a sequence of digits.<br>\n",
    "**. (period)** matches any single character e.g. *beg.n* would match *begin, beg'n, begun* etc.<br>\n",
    "If we want to find a specific sentence, the sequence is more or less the caret sign + sentence + the dollar sign.\n",
    "**\\b** matches word boundary e.g. **\\b99\\b** would match *99* but not *299* and **\\B** matches a non-boundary.<br>\n",
    "\n",
    "### Disjunction, Grouping and Precedence\n",
    "Disjunction operator **( | )**\n",
    "Regular Expression *cat|dog* matches either string *cat* or *dog* in a string. Enclosing the disjunction operator within parentheses makes it act like a single character e.g. *gupp(y|ies)*.<br>\n",
    "Instead of Column [0-9] * to match *Column 1 Column 2 Column 3* we should use parentheses around the whole expression e.g. (Column [0-9] *)* to properly match any number of strings containing Column followed by a number.<br>\n",
    "Operator precedence in regular expressions\n",
    "**Parentheses > Counters ( * ? + {} ) > Sequence and anchors ( *the* + ? {} ) > Disjunction ( | )**<br>\n",
    "Patterns are **greedy** meaning that they cover as much string as possible as long as we tell them not to be by using operators * ? and +?.<br>\n",
    "\n",
    "### Examples\n",
    "\n",
    "To match prices such as \\\\$799.99, $200 etc.\n",
    "\n",
    "**(ˆ|\\W)\\\\$[0-9]{0,3}(\\.[0-9][0-9])?\\b** would match \\\\$199.99, \\\\$199, \\\\$723.54 etc.<br> \n",
    "*(ˆ|\\W)*: Beginning of string or non-alphabetic character<br>\n",
    "*\\\\$*: means the actual dollar sign and not the end of string<br>\n",
    "*[0-9]{0,3}*: would match sequence of digits such as 23, 359 but not 4932<br>\n",
    "*(\\.[0-9][0-9])?\\b*: parentheses and question mark to make the cents optional. \\. to match the dot and not wildcard. \\b to specify a boundary.<br>\n",
    "\n",
    "**\\b[6-9]+ *(GHz|[Gg]igahertz)\\b** would match *6 GHz*, *8 Gigahertz* etc. but not 5 GHz or 9 ghz.<br>\n",
    "\\b[6-9]+ *: to match a boundary followed by a digit 6 through 9 followed by any number of spaces<br>\n",
    "(GHz|[Gg]igahertz)\\b: to match strings GHz, Gigahertz or gigahertz.<br>\n",
    "\n",
    "### Exercise\n",
    "\n",
    "**\\b[0-9]+(\\.[0-9]+)? \\*(GB|[Gg]igabytes?)\\b adjust this RegEx to only match more than 500 GB.**\n",
    "\n",
    "### Answer\n",
    "\n",
    "**\\b([5-9]\\d{2}|[1-9]\\d{3,})(\\.[0-9]{2})?\\s?(GB|[Gg]igabytes)\\b**\n",
    "\n",
    "### More Operators\n",
    "\n",
    "**\\d**: Any digit<br>\n",
    "**\\D**: Any non-digit<br>\n",
    "**\\w**: Any alphanumeric character ( [a-zA-Z0-9_] )<br>\n",
    "**\\W**: Any non-alphanumeric character<br>\n",
    "**\\s**: Whitespace<br>\n",
    "**\\S**: Non-whitespace<br>\n",
    "\n",
    "### Regular Expression Substitution, Capture Groups and Eliza\n",
    "\n",
    "**s/regexp1/pattern/** is used for substitution in python and Unix systems.<br>\n",
    "<br>\n",
    "**Example**: s/colour/color would substitute all *colour* to *color*.<br>\n",
    "s/([0-9]+)/<\\1>/ would put angle brackets around all integers in text. Parentheses use is important here to use the number operator (\\1).\n",
    "\n",
    "**Capture groups**: parentheses surrounding a pattern (\\.\\*). Every time a capture group is used, the resulting match is stored in a numbered register. If we want to use this for grouping but don't want to put the result in a numbered register, then we can use a non-capturing group (**?:**)<br>\n",
    "**Example:** *(?:some|a few) (people|cats) like some \\1* expression would match *some cats like some cats* but not *some cats like some a few*<br>\n",
    "\n",
    "### Lookahead Assertions\n",
    "\n",
    "Lookahead assertions are used when we want to see if a pattern matches but we don't want to advance the match cursor so that we can deal with the pattern when it occurs. (?= pattern) is true if pattern occurs but is zero-width meaning that the match pointer doesn't advance. (?! pattern) is used for a **negative lookahead**.<br>\n",
    "\n",
    "**Example**: Suppose we want to match, at the beginning of a line, any word that doesn't start with 'Volcano':<br>\n",
    "<br>\n",
    "ˆ(?!Volcano)[A-Za-z]+ expression can be used to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "**Corpus (plural: corpora)**: a computer-readable collection of text or speech.<br>\n",
    "**Types**: are the set of distinct words in a corpus.<br>\n",
    "**Tokens**: are the total number N of running words.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora\n",
    "\n",
    "World has about 7097 languages and the algorithms for NLP should be written to cover as many of them as possible. Also, one language can have different dialects spoken by different people e.g. *talmabout* (African American Vernacular English) vs. *talking about* (Standard American English).<br>\n",
    "<br>\n",
    "Code switching, which means using multiple languages when speaking or writing is also important in language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "\n",
    "At least three tasks must be carried out in order to normalize the text before any natural language processing is applied:<br>\n",
    "    1. Segmenting/tokenizing words from running text\n",
    "    2. Normalizing word formats\n",
    "    3. Segmenting sentences in a running text.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unix tools for crude tokenization and normalization\n",
    "\n",
    "Using a Unix system to count words in a corpus:<br>\n",
    "<br>\n",
    "    tr 'A-Za-z' '\\n' < filename.txt | tr A-Z a-z | sort | uniq -c | sort -n -r<br>  \n",
    "**tr 'A-Za-z' '\\n' < filename.txt**: Tokenizes each word in the corpus<br>\n",
    "**tr A-Z a-z**: Converts all uppercase characters to lowercase<br>\n",
    "**sort**: Sorts the words alphabetically<br>\n",
    "**uniq -c**: Counts the unique words. Output is something like *'23 a'*<br>\n",
    "**sort -n -r**: Sorts the list again but -n option is added for a numerical sorting rather than alphabetic and -r option is for reverse (highest-to-lowest)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization and Normalization\n",
    "\n",
    "**Named entity detection**: Task of detecting names, dates and organizations etc.\n",
    "\n",
    "##### NB! Linguistic Data Consortium is a useful source to obtain linguistic data: https://www.ldc.upenn.edu/\n",
    "\n",
    "*Penn Treebank Tokenization* standard is one of the most commonly used tokenization standards.\n",
    "\n",
    "<span style='color:red'>**Note to self**: Information retrieval is what I will mostly deal with.</span><br>\n",
    "For tasks such as speech recognition and information retrieval, given data is mostly converted to lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Segmentation in Chinese: the MaxMatch algorithm\n",
    "\n",
    "In Chinese, words are composed of characters known as **hanzi**.\n",
    "\n",
    "**MaxMatch algorithm written in python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['他', '特别', '喜欢', '北京烤鸭']\n",
      "['we', 'canon', 'l', 'y', 'see', 'ash', 'ort', 'distance', 'ahead']\n"
     ]
    }
   ],
   "source": [
    "def max_match(sentence, dictionary):\n",
    "    # If sentence is empty, return an empty list\n",
    "    if not sentence:\n",
    "        return []\n",
    "    #\n",
    "    for i in range(len(sentence)-1, 0, -1):\n",
    "        first_word = sentence[0:i+1]\n",
    "        remainder = sentence[i+1:len(sentence)]\n",
    "        if first_word in dictionary:\n",
    "            return [first_word] + max_match(remainder, dictionary)\n",
    "    first_word = sentence[0]\n",
    "    remainder = sentence[1:]\n",
    "    return [first_word] + max_match(remainder,dictionary)\n",
    "\n",
    "\n",
    "dictionary_chinese = ['他', '特别', '喜欢', '北京烤鸭']\n",
    "dictionary_english = ['we', 'can', 'canon', 'only', 'see', 'ash', 'a', 'short',\n",
    "                      'ort', 'distance', 'stance', 'ahead', 'head']\n",
    "\n",
    "print(max_match(\"他特别喜欢北京烤鸭\", dictionary_chinese))\n",
    "print(max_match(\"wecanonlyseeashortdistanceahead\", dictionary_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxMatch algorithm works very well with chinese but not with English as can be seen from the example.<br>\n",
    "\n",
    "We can quantify how well a *segmenter* works by using a metric called **word error rate**. To find it, we compare our output to a perfect hand-segmented (*'gold'*) sentence to see how many words differ.<br>\n",
    "\n",
    "Even in Chinese, MaxMatch has problems dealing with *unknown words* (words not in the dictionary) or *different genres*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsing words: Lemmatization and Stemming\n",
    "\n",
    "**Lemmatization** is the task of determining that two words have the same root, despite their surface differences e.g. the words *am*, *is* and *are* all have the same lemma *be*.<br>\n",
    "\n",
    "**Morphology** is the study of the way words are built up from smaller meaning-bearing unit calles **morphemes**. Two broad classes of morphemes can be distinguished: **stems** and **affixes** e.g. the word *cats* has two morphemes *cat* and *s*.\n",
    "\n",
    "#### The Porter Stemmer\n",
    "\n",
    "**Stemming** is a more naive version of lemmatization, which mainly chops off word-final affixes.<br>\n",
    "One of the most commonly used stemming algorithms is Porter.<br>\n",
    "Stemming and lemmatization has many side-benefits such as it helps systems deal with unknown words more easily. Let's say the training corpus has words *low* and *lower* in it but not *lowest*. Stemming all these words to *low* might solve our problem, however this creates problems related to other fields of NLP e.g. POS (part-of-speech) tagging. \n",
    "\n",
    "#### Byte-Pair Encoding\n",
    "\n",
    "The algorithm begins with a set of symbols equal to set of characters. Each word is represented as a sequence of character plus a end-of-word symbol. At each iteration, the most frequent pairs are found and merged into a new symbol e.g. ('A', 'B') becomes ('AB').<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'r')\n",
      "('er', '</w>')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('new', 'er</w>')\n",
      "('low', '</w>')\n"
     ]
    }
   ],
   "source": [
    "# Byte Pair Encoding (BPE) algorithm as implemented in Sennrich et al. (2016)\n",
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "\n",
    "vocab = {'l o w </w>': 5, 'l o w e s t </w>': 2,\n",
    "        'n e w e r </w>': 6, 'w i d e r </w>': 3, 'n e w </w>': 2}\n",
    "num_merges = 8\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n",
    "\n",
    "The most useful cues for sentence segmentation are punctuations. Punctuations such as question mark or exclamation mark are less ambiguous, whereas dot '.' is more ambiguous as it can be used in places such as *Mr.* or *Inc.* The ambiguity of the punctuation mark at the end of the first sentence can be challenging to handle while processing text. Most recent methods of sentence segmentation use machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Edit Distance\n",
    "\n",
    "**Edit distance** gives us a way to quantify how similar two strings are. **Minimum edit distance** is defined as the minimum required editing to make one string same as the other. The gap between *intention* and *execution* is 5.<br>\n",
    "d: deletion, s: substitution, i: insertion\n",
    "GAP: 5<br>\n",
    " I N T E * N T I O N<br>\n",
    " | | | | | | | | | |<br>\n",
    "\\* E X E C U T I O N<br>\n",
    " d s s   i s<br>\n",
    "GAP: 1<br>\n",
    " G * R A F F E<br>\n",
    " | | | | | | |\n",
    " G I R A F F E<br>\n",
    "   i<br>\n",
    "\n",
    "In **Levenshtein** method, each of these three operations has a weight of 1.<br> \n",
    "Minimum edit distance can be used to determine what the user meant when there are typos in what they typed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### The Minimum Edit Distance Algorithm\n",
    " \n",
    " As the number of all possible edits is enormous, we need a different approach to find the shortest path from one string to another. So rather than recomputing possibilities, we could remember the shortest path each time we saw it. Dynamic programming tends to solve problems by finding solution to various sub-problems.<br>\n",
    " \n",
    "**D(i,j) = min(\\[D(i-1, j) + deletion_cost(source[i]), D(i,j-1) + insertion_cost(target[j]), D(i-1, j-1) 2 if source[i] != target[j], 0 if source[i] = target[j])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def min_edit_distance(source, target, m, n):\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if source[m-1] == target[n-1]:\n",
    "        return min_edit_distance(source, target, m-1, n-1)\n",
    "\n",
    "    return 1 + min( min_edit_distance(source, target, m, n-1),\n",
    "                    min_edit_distance(source, target, m-1, n),\n",
    "                    min_edit_distance(source, target, m-1, n-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source = \"yesdkc\"\n",
    "target = \"hellow\"\n",
    "print(min_edit_distance(source, target, len(source), len(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This chapter introduced RegEx, text normalization tasks such as tokenizing, lemmatization, stemming etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
